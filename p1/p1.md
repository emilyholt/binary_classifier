## 1A : Dataset Exploration

|                               | Training Set | Validation Set |
| ----------------------------- | ------------ | -------------- |
| Number of examples, total     | 9817         | 1983           |
| Number of positive examples   | 4913         | 1036           |
| Fraction of positive examples | 0.500        | 0.522          |

## Figure 1B : Assess Loss and Error vs. Training Iterations

See figure 1B for the results
![Two charts plotting loss and error as a function of the number of training iterations used in fitting the model to the training data/labels](1B_loss_error.png)

## SA 1B : Below the plots, discuss the results you are seeing; what do they show, and why?

These plots illustrate the performance of our models as the number of training iterations increases. We can see that in both the error and log-loss plots, the training error continues to decrease as the number of iterations increases. However, we observe that between 5-10 iterations we hit a sweet-spot in validation-set performance. Beyond 10 iterations we start to see large increases in error and log-loss for our validation set, indicating that our model suffers from overfitting at larger iterations.
